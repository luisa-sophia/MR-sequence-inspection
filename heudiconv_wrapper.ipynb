{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import pandas as pd\n",
    "import heudiconv\n",
    "import sys\n",
    "import config.core as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heudiconv wrapper script\n",
    "This script runs heudiconv on raw MR data (dicom files), generating an overview tsv file that lists metadata for each image type (e.g. sequence that was used to generate it). <br>\n",
    "Output files can be found in the ./data directory after running the script.\n",
    "\n",
    "Requirements:\n",
    "- *config vars* as specified in config module (will automatically be imported after specifying data root),\n",
    "    - DICOM_ROOT_PATH\n",
    "    - DCM_PATTERN\n",
    "\n",
    "Output:\n",
    "- *.heudiconv folder* containing the summaries, one for each session (most importantly: containing the dicominfo.tsv file with the sequence infos, or a modification version thereof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration initialised with base directory: /home/luisass/Work\n"
     ]
    }
   ],
   "source": [
    "#cfg.initialise_config(\"/mnt/labdrive\", force = True)\n",
    "cfg.initialise_config(keyword = \"Work\", force = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT FILES to '/home/luisass/Work/MR/MR-sequence-inspection/data'\n",
      "\n",
      "=== CONFIG VARIABLES ============================================================\n",
      "=== from /home/luisass/Work/MR/MR-sequence-inspection/config/cfgvars.py\n",
      "=================================================================================\n",
      "DCM_PATTERN               = /mnt/labdrive/this/is/a/path/to/root/{subject}/SCANS/*/DICOM/*.dcm\n",
      "DICOM_ROOT_PATH           = /mnt/labdrive/this/is/a/path/to/root\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "IGNORE_PILOTS = True #ignore the scans that have the word \"pilot\" in their name\n",
    "IGNORE_HIDDEN = True #ignore hidden folders starting with a '.'. E.g. path/to/.thisisafoldertoignore\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "print(f\"OUTPUT FILES to '{OUTPUT_DIR}'\")\n",
    "\n",
    "cfg.print_config_vars([\"DCM_PATTERN\", \"DICOM_ROOT_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/data\n"
     ]
    }
   ],
   "source": [
    "#create output path if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok = True)\n",
    "    print(f\"Created {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring Pilots: True\n",
      "Ignoring hidden folders: True\n",
      "\n",
      "Found following sessions at path /home/luisass/this/is/a/path/to/root:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_dirs = list()\n",
    "for root, dirs, files in os.walk(cfg.DICOM_ROOT_PATH, topdown=True):\n",
    "    #exclude hidden folders and pilot scans\n",
    "    if IGNORE_HIDDEN:\n",
    "        dirs[:] = [d for d in dirs if d[0] != '.']\n",
    "    if IGNORE_PILOTS:\n",
    "        dirs[:] = [d for d in dirs if \"pilot\" not in d.lower()]\n",
    "    for dir in dirs:\n",
    "        if dir == \"SCANS\":\n",
    "            session_dirs.append(root)\n",
    "            dirs.remove(\"SCANS\")\n",
    "\n",
    "sessions = [os.path.basename(p) for p in session_dirs]\n",
    "\n",
    "print(f\"Ignoring Pilots: {IGNORE_PILOTS}\\nIgnoring hidden folders: {IGNORE_HIDDEN}\\n\")\n",
    "print(f\"Found following sessions at path {cfg.DICOM_ROOT_PATH}:\")\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: heudiconv [-h] [--version]\n",
      "                 [-d DICOM_DIR_TEMPLATE | --files [FILES ...]]\n",
      "                 [-s [SUBJS ...]] [-c {dcm2niix,none}] [-o OUTDIR]\n",
      "                 [-l LOCATOR] [-a CONV_OUTDIR] [--anon-cmd ANON_CMD]\n",
      "                 [-f HEURISTIC] [-p] [-ss SESSION]\n",
      "                 [-b [BIDSOPTION1 [BIDSOPTION2 ...]]] [--overwrite]\n",
      "                 [--datalad] [--dbg]\n",
      "                 [--command {heuristics,heuristic-info,ls,populate-templates,sanitize-jsons,treat-jsons,populate-intended-for}]\n",
      "                 [-g {studyUID,accession_number,all,custom}] [--minmeta]\n",
      "                 [--random-seed RANDOM_SEED] [--dcmconfig DCMCONFIG]\n",
      "                 [-q {SLURM,None}] [--queue-args QUEUE_ARGS]\n",
      "\n",
      "Example: heudiconv -d 'rawdata/{subject}' -o . -f heuristic.py -s s1 s2 s3\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  -d DICOM_DIR_TEMPLATE, --dicom_dir_template DICOM_DIR_TEMPLATE\n",
      "                        Location of dicomdir that can be indexed with subject\n",
      "                        id {subject} and session {session}. Tarballs (can be\n",
      "                        compressed) are supported in addition to directory.\n",
      "                        All matching tarballs for a subject are extracted and\n",
      "                        their content processed in a single pass. If multiple\n",
      "                        tarballs are found, each is assumed to be a separate\n",
      "                        session and the --ses argument is ignored. Note that\n",
      "                        you might need to surround the value with quotes to\n",
      "                        avoid {...} being considered by shell\n",
      "  --files [FILES ...]   Files (tarballs, dicoms) or directories containing\n",
      "                        files to process. Cannot be provided if using\n",
      "                        --dicom_dir_template.\n",
      "  -s [SUBJS ...], --subjects [SUBJS ...]\n",
      "                        List of subjects - required for dicom template. If not\n",
      "                        provided, DICOMS would first be \"sorted\" and subject\n",
      "                        IDs deduced by the heuristic.\n",
      "  -c {dcm2niix,none}, --converter {dcm2niix,none}\n",
      "                        Tool to use for DICOM conversion. Setting to \"none\"\n",
      "                        disables the actual conversion step -- useful for\n",
      "                        testing heuristics.\n",
      "  -o OUTDIR, --outdir OUTDIR\n",
      "                        Output directory for conversion setup (for further\n",
      "                        customization and future reference. This directory\n",
      "                        will refer to non-anonymized subject IDs.\n",
      "  -l LOCATOR, --locator LOCATOR\n",
      "                        Study path under outdir. If provided, it overloads the\n",
      "                        value provided by the heuristic. If --datalad is\n",
      "                        enabled, every directory within locator becomes a\n",
      "                        super-dataset thus establishing a hierarchy. Setting\n",
      "                        to \"unknown\" will skip that dataset.\n",
      "  -a CONV_OUTDIR, --conv-outdir CONV_OUTDIR\n",
      "                        Output directory for converted files. By default this\n",
      "                        is identical to --outdir. This option is most useful\n",
      "                        in combination with --anon-cmd.\n",
      "  --anon-cmd ANON_CMD   Command to run to convert subject IDs used for DICOMs\n",
      "                        to anonymized IDs. Such command must take a single\n",
      "                        argument and return a single anonymized ID. Also see\n",
      "                        --conv-outdir.\n",
      "  -f HEURISTIC, --heuristic HEURISTIC\n",
      "                        Name of a known heuristic or path to the Python script\n",
      "                        containing heuristic.\n",
      "  -p, --with-prov       Store additional provenance information. Requires\n",
      "                        python-rdflib.\n",
      "  -ss SESSION, --ses SESSION\n",
      "                        Session for longitudinal study_sessions. Default is\n",
      "                        None.\n",
      "  -b [BIDSOPTION1 [BIDSOPTION2 ...]], --bids [BIDSOPTION1 [BIDSOPTION2 ...]]\n",
      "                        Flag for output into BIDS structure. Can also take\n",
      "                        BIDS-specific options, e.g., --bids notop. The only\n",
      "                        currently supported options is \"notop\", which skips\n",
      "                        creation of top-level BIDS files. This is useful when\n",
      "                        running in batch mode to prevent possible race\n",
      "                        conditions.\n",
      "  --overwrite           Overwrite existing converted files.\n",
      "  --datalad             Store the entire collection as DataLad dataset(s).\n",
      "                        Small files will be committed directly to git, while\n",
      "                        large to annex. New version (6) of annex repositories\n",
      "                        will be used in a \"thin\" mode so it would look to\n",
      "                        mortals as just any other regular directory (i.e. no\n",
      "                        symlinks to under .git/annex). For now just for BIDS\n",
      "                        mode.\n",
      "  --dbg                 Do not catch exceptions and show exception traceback.\n",
      "  --command {heuristics,heuristic-info,ls,populate-templates,sanitize-jsons,treat-jsons,populate-intended-for}\n",
      "                        Custom action to be performed on provided files\n",
      "                        instead of regular operation.\n",
      "  -g {studyUID,accession_number,all,custom}, --grouping {studyUID,accession_number,all,custom}\n",
      "                        How to group dicoms (default: by studyUID).\n",
      "  --minmeta             Exclude dcmstack meta information in sidecar jsons.\n",
      "  --random-seed RANDOM_SEED\n",
      "                        Random seed to initialize RNG.\n",
      "  --dcmconfig DCMCONFIG\n",
      "                        JSON file for additional dcm2niix configuration.\n",
      "\n",
      "Conversion submission options:\n",
      "  -q {SLURM,None}, --queue {SLURM,None}\n",
      "                        Batch system to submit jobs in parallel.\n",
      "  --queue-args QUEUE_ARGS\n",
      "                        Additional queue arguments passed as a single string\n",
      "                        of space-separated Argument=Value pairs.\n"
     ]
    }
   ],
   "source": [
    "#not necessary to run this cell, just for info on the heudiconv tool :)\n",
    "!heudiconv --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... nothing to process!\n"
     ]
    }
   ],
   "source": [
    "#============================\n",
    "FORCE_HEUDICONV = False #change this flag if you want to re-run all subjects, even if a tsv file already exists\n",
    "#============================\n",
    "\n",
    "#check which subjects \n",
    "dicominfo_file_paths = glob.glob(r'**/dicominfo*.tsv', root_dir= OUTPUT_DIR, recursive= True, include_hidden=True)\n",
    "already_processed_sessions = [os.path.basename(os.path.dirname(os.path.dirname(p))) for p in dicominfo_file_paths]\n",
    "\n",
    "if len(sessions) == 0:\n",
    "    print(\"... nothing to process!\")\n",
    "else:\n",
    "    # run heudiconv on each session\n",
    "    for i, session in enumerate(sessions):\n",
    "        if session in already_processed_sessions and not FORCE_HEUDICONV:\n",
    "            print(f\"=== SESSION ({i+1}/{len(sessions)}): {session:>20s} --> dicominfo tsv file exists. Won't run heudiconv.\")\n",
    "        else:\n",
    "            print(f\"==== SESSION ({i+1}/{len(sessions)}): {session:>20s} --> RUNNING HEUDICONV!\")\n",
    "            cmd = rf'heudiconv -d {cfg.DCM_PATTERN} -s {session} -o {OUTPUT_DIR} -c none -f convertall'\n",
    "            print(f\"Command: {cmd}\")\n",
    "\n",
    "            # merge stderr into stdout so we only read one stream\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                shell=True\n",
    "            )\n",
    "\n",
    "            # Stream all output live\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                print(line.strip())\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "            # Wait for process to finish and get exit code\n",
    "            returncode = process.wait()\n",
    "            if returncode != 0:\n",
    "                print(f\"--- heudiconv FAILED for session {session} with exit code {returncode} ---\")\n",
    "\n",
    "    print(\"\\n... success, all sessions processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete sensitive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicominfo_file_paths = glob.glob(r'**/dicominfo*.tsv', root_dir= OUTPUT_DIR, recursive= True, include_hidden=True)\n",
    "dicominfo_file_paths = [os.path.join(OUTPUT_DIR, p) for p in dicominfo_file_paths]\n",
    "\n",
    "#to read out scan IDs of dicominfos (just for sanity check)\n",
    "dicominfo_scan_IDs = list()\n",
    "\n",
    "columns_to_delete = [\"patient_age\", \"patient_sex\"]\n",
    "for info_file_path in dicominfo_file_paths:\n",
    "    if os.path.exists(info_file_path):\n",
    "        dicominfo_scan_IDs.append(os.path.basename(os.path.dirname(os.path.dirname(info_file_path))))\n",
    "        tsv_file = pd.read_csv(info_file_path, sep='\\t', encoding='latin-1', on_bad_lines='skip')\n",
    "        if any(c in tsv_file.columns for c in columns_to_delete):\n",
    "            tsv_file.drop(columns=columns_to_delete, inplace=True, errors=\"ignore\")\n",
    "            # Write the modified DataFrame back to a new TSV file\n",
    "            new_file_path = os.path.join(os.path.dirname(info_file_path), cfg.DICOMINFO_TSV_NAME)\n",
    "            tsv_file.to_csv(new_file_path, sep='\\t', index=False)\n",
    "            print(f\"deleted sensitive columns for {info_file_path}\")\n",
    "            os.remove(info_file_path)\n",
    "        else:\n",
    "            print(f\"No sensitive columns found for {info_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing data or missing info sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all scans a dicominfo sheet was found, and for all dicominfo sheets a scan was found.\n"
     ]
    }
   ],
   "source": [
    "if sorted(sessions) != sorted(dicominfo_scan_IDs):\n",
    "    no_info = list(set(sessions) - set(dicominfo_scan_IDs))\n",
    "    no_data = list(set(dicominfo_scan_IDs) - set(sessions))\n",
    "    print(f\"Existing scans that have no dicominfo.tsv: {no_info}\")\n",
    "    print(f\"Existing dicominfo.tsv where no scan is found: {no_data}\")\n",
    "    raise ValueError(\"Found mismatch in found Scan data and existing summary sheets. See print output above.\")\n",
    "else:\n",
    "    print(\"For all scans a dicominfo sheet was found, and for all dicominfo sheets a scan was found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsc_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
