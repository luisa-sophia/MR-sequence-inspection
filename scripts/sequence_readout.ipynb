{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import inspect\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "%reload_ext autoreload\n",
    "from config_paths_and_vars import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_id(path):\n",
    "    return os.path.normpath(path).split(\"info\")[0].split(os.sep)[-2]\n",
    "\n",
    "def custom_sort(subject):\n",
    "    \"\"\"\n",
    "    Custom sorting function based on subject identifiers and their numbers.\n",
    "    Sorts identifiers starting with TRD before any others.\n",
    "\n",
    "    Args:\n",
    "        subject (str): A string representing the subject identifier.\n",
    "\n",
    "    Returns:\n",
    "        str: A modified string used for sorting the subject.\n",
    "\n",
    "    Example:\n",
    "        >>> custom_sort('TRD7T1234_moretext')\n",
    "        '1234'\n",
    "        >>> custom_sort('subject_021_text')\n",
    "        '_text'\n",
    "    \"\"\"\n",
    "    if subject.startswith(\"TRD\"):\n",
    "        return subject.split(\"7T\")[1][:4]\n",
    "    else:\n",
    "        return \"_\" + subject.split(\"021_\", maxsplit = 1)[1][1:]\n",
    "    \n",
    "def print_modality_conditions(lambdafunc):\n",
    "    if isinstance(lambdafunc, dict):\n",
    "        for key, func in lambdafunc.items():\n",
    "            try:\n",
    "                source = inspect.getsource(func).strip()\n",
    "            except OSError:\n",
    "                source = \"<lambda source unavailable>\"\n",
    "            print(f'\"{key}\": {source}')\n",
    "    elif callable(lambdafunc):\n",
    "        source = inspect.getsource(lambdafunc).strip()\n",
    "        print(source)\n",
    "    else:\n",
    "        raise ValueError(\"You must paste either a lambda fuction or a dictionary of lambdafunctions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Info overview\n",
    "This script is dedicated to giving an overview which sequences were run during the scanning session of a group of scans (e.g. for all TGA participants). <br>\n",
    "Make sure all participants have a dicominfo tsv file (you can create those by referring to the heudiconv_wrapper.ipynb in this directory). <br>\n",
    "So it must be run AFTER the heudiconv_wrapper script. <br>\n",
    "\n",
    "Requirements:\n",
    "- *dicominfo tsv files* - for each session that should be considered, as generated by heudiconv. See heudiconv_wrapper notebook in this directory. \n",
    "- *config vars* as specified in config_paths_and_vars.py (will automatically be imported), e.g.\n",
    "    - sorting rules - to classify scans (modality conditions).\n",
    "    - path template - to search for spectroscopy scans (as these are not captured by heudiconv).\n",
    "    - details about classifying fmri data into blocks\n",
    "\n",
    "Output:\n",
    "- *session_sequence_summary.tsv* - a tsv file that indicates which sequences were run for each session\n",
    "- *sequence_type_count.tsv* - a tsv file that lists all sequences run count across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT FILES to '/home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/scripts/data':\n",
      "- PROJECT1_scan_sessions_sequence_summary.tsv\n",
      "- PROJECT1_scan_sessions_sequence_type_count.tsv\n",
      "\n",
      "=== CONFIG VARIABLES ============================================================\n",
      "=== see /home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/config_paths_and_vars.py\n",
      "=================================================================================\n",
      "IDENTIFIER                = PROJECT1\n",
      "DICOMINFO_TSV_NAME        = dicominfo_mod.tsv\n",
      "SPECTRO_TEMPLATE          = /home/luisass/ishere/*/*/MRS-data\n",
      "NUM_TRS_ONE_FMRI_BLOCK    = 418\n",
      "FMRI_BLOCK_THRESHOLD      = 0.4\n",
      "=================================================================================\n",
      "MODALITY_CONDITIONS=\n",
      "\"T1w\": \"T1w\": lambda x: \"t1\" in x and \"UNI\" in x and \"0.75\" in x,\n",
      "\"T2w\": \"T2w\": lambda x: \"t2\" in x,\n",
      "\"fMRI\": \"fMRI\": lambda x: \"face\" in x and \"ap\" in x and not \"SBRef\" in x,\n",
      "\"DWI\": \"DWI\": lambda x: \"resolve_COR\" in x,\n",
      "\"DTI\": \"DTI\": lambda x: \"dti\" in x\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "HEUDICONV_INFO_ROOT = OUTPUT_DIR\n",
    "SUMMARY_FILENAME = fr'{IDENTIFIER}_scan_sessions_sequence_summary.tsv'\n",
    "COUNT_FILENAME = fr'{IDENTIFIER}_scan_sessions_sequence_type_count.tsv'\n",
    "\n",
    "print(f\"OUTPUT FILES to '{OUTPUT_DIR}':\\n- {SUMMARY_FILENAME}\\n- {COUNT_FILENAME}\")\n",
    "\n",
    "print_config_vars([\"IDENTIFIER\", \"DICOMINFO_TSV_NAME\", \"SPECTRO_TEMPLATE\", \"NUM_TRS_ONE_FMRI_BLOCK\", \"FMRI_BLOCK_THRESHOLD\"])\n",
    "print(\"MODALITY_CONDITIONS=\")\n",
    "print_modality_conditions(get_config_vars()[\"MODALITY_CONDITIONS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output path if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.path.makedirs(OUTPUT_DIR, exist_ok = True)\n",
    "    print(f\"Created {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sessions found in total, with root '/home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/scripts/data'.\n",
      "\n",
      "0 sessions with spectroscopy scan, with template /home/luisass/ishere/*/*/MRS-data.\n"
     ]
    }
   ],
   "source": [
    "tsv_file_paths = glob.glob(fr\"**/{DICOMINFO_TSV_NAME}\", root_dir= HEUDICONV_INFO_ROOT, recursive=True, include_hidden=True)\n",
    "tsv_file_paths = [os.path.join(HEUDICONV_INFO_ROOT, p) for p in tsv_file_paths]\n",
    "\n",
    "#exclude scan IDs if specified\n",
    "tsv_file_paths = [p for p in tsv_file_paths if not any(exclusion_ID in p for exclusion_ID in EXCLUDE_SCAN_IDS)]\n",
    "\n",
    "print(f\"{len(tsv_file_paths)} sessions found in total, with root '{HEUDICONV_INFO_ROOT}'.\")\n",
    "\n",
    "spectro_paths = glob.glob(SPECTRO_TEMPLATE)\n",
    "IDs_w_spectro = [os.path.basename(os.path.dirname(os.path.dirname(p))) for p in spectro_paths]\n",
    "print(f\"\\n{len(IDs_w_spectro)} sessions with spectroscopy scan, with template {SPECTRO_TEMPLATE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions found:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_IDs = [get_session_id(p) for p in tsv_file_paths]\n",
    "session_IDs = sorted(session_IDs, key=custom_sort)\n",
    "print(\"Sessions found:\\n\" + \"\\n\".join(session_IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare summary dataframe\n",
    "sequence_overview_df = pd.DataFrame(0, index=range(len(session_IDs)), columns=[\"Session_ID\", \"T1w\", \"T2w\", \"fMRI\", \"fMRI_blocks\",\"fMRI_num_TRs\", \"DWI\", \"DTI\", \"Spectro\"])\n",
    "sequence_overview_df[\"fMRI_blocks\"] = sequence_overview_df[\"fMRI_blocks\"].astype(float)\n",
    "sequence_overview_df[\"fMRI_num_TRs\"] = sequence_overview_df[\"fMRI_num_TRs\"].astype(str) #because we are gonna have it as a list in the dataframe\n",
    "sequence_overview_df[\"Session_ID\"] = session_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>T1w</th>\n",
       "      <th>T2w</th>\n",
       "      <th>fMRI</th>\n",
       "      <th>fMRI_blocks</th>\n",
       "      <th>fMRI_num_TRs</th>\n",
       "      <th>DWI</th>\n",
       "      <th>DTI</th>\n",
       "      <th>Spectro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Session_ID, T1w, T2w, fMRI, fMRI_blocks, fMRI_num_TRs, DWI, DTI, Spectro]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect all occuring series descriptions\n",
    "all_series_descriptions = list()\n",
    "#iterate through found dicom info tsvs\n",
    "for tsv_file_path in tsv_file_paths:\n",
    "    tsv_file = pd.read_csv(tsv_file_path, sep='\\t', encoding='latin-1', on_bad_lines='skip')\n",
    "    series_description = tsv_file[\"series_description\"].values\n",
    "    all_series_descriptions.extend(series_description)\n",
    "    #session id and index in the overview df\n",
    "    cur_id = get_session_id(tsv_file_path)\n",
    "    sess_ID_index = sequence_overview_df[sequence_overview_df[\"Session_ID\"] == cur_id].index\n",
    "    #sometimes we have more than one fMRI run, so make a list\n",
    "    fMRI_run_TRs = list()\n",
    "    #now check all registered sequences in the current tsv file\n",
    "    for index, row in tsv_file.iterrows():\n",
    "        cur_series = row[\"series_description\"]\n",
    "        #if not null\n",
    "        if cur_series == cur_series:\n",
    "            #check for modalities\n",
    "            for modality, condition in MODALITY_CONDITIONS.items():\n",
    "                if condition(cur_series):\n",
    "                    sequence_overview_df.loc[sess_ID_index, modality] = 1\n",
    "                    break #exclusive\n",
    "\n",
    "            if MODALITY_CONDITIONS[\"fMRI\"](cur_series):\n",
    "                num_TRs = tsv_file.loc[index, \"dim4\"]\n",
    "                fMRI_run_TRs.append(num_TRs)\n",
    "\n",
    "    #how many fMRI blocks?           \n",
    "    if fMRI_run_TRs:          \n",
    "        if any(fMRI_run_TRs > FMRI_BLOCK_THRESHOLD * num_TRs):\n",
    "            num_blocks = 0.0\n",
    "            for num_TRs in fMRI_run_TRs:\n",
    "                if num_TRs / NUM_TRS_ONE_FMRI_BLOCK >= FMRI_BLOCK_THRESHOLD:\n",
    "                    num_blocks = num_blocks + np.round(num_TRs / NUM_TRS_ONE_FMRI_BLOCK, 1)\n",
    "            sequence_overview_df.loc[sess_ID_index, \"fMRI_blocks\"] = num_blocks\n",
    "        sequence_overview_df.loc[sess_ID_index, \"fMRI_num_TRs\"] = str(\", \".join([str(int(tr)) for tr in fMRI_run_TRs]))\n",
    "\n",
    "    if cur_id in IDs_w_spectro:\n",
    "        sequence_overview_df.loc[sess_ID_index, \"Spectro\"] = 1\n",
    "    \n",
    "\n",
    "sequence_overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_description</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [series_description, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_counts = pd.Series(all_series_descriptions).value_counts()\n",
    "description_counts = description_counts.reset_index()\n",
    "description_counts.columns = [\"series_description\", \"count\"]\n",
    "description_counts[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_dfs_different(df_1, df_2):\n",
    "    if df_1.shape != df_2.shape:\n",
    "        return True\n",
    "    \n",
    "    return not df_1.equals(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes detected. Attempting update...\n",
      "Updated table at '/home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/scripts/data/PROJECT1_scan_sessions_sequence_summary.tsv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "UPDATE = True #should existing file be updated? \n",
    "# ==============================================\n",
    "\n",
    "sequence_summary_path = os.path.abspath(os.path.join(OUTPUT_DIR, SUMMARY_FILENAME))\n",
    "\n",
    "#create new file if not existing\n",
    "if not os.path.exists(sequence_summary_path):\n",
    "    updated_df = sequence_overview_df.copy()\n",
    "    updated_df[\"Comments\"] = \"\"\n",
    "    updated_df.to_csv(sequence_summary_path, sep = \"\\t\", index = False)\n",
    "    print(f\"Created file at '{sequence_summary_path}'\")\n",
    "\n",
    "#or possibly update existing one\n",
    "else:\n",
    "\n",
    "    old_sequence_overview_df = pd.read_csv(sequence_summary_path, sep = \"\\t\")\n",
    "    cols_without_comments = list(old_sequence_overview_df.columns)\n",
    "    cols_without_comments.remove(\"Comments\")\n",
    "\n",
    "    if are_dfs_different(sequence_overview_df, old_sequence_overview_df[cols_without_comments]):\n",
    "        print(\"Changes detected. Attempting update...\")\n",
    "        if UPDATE:\n",
    "            #update the file\n",
    "            comment_column = old_sequence_overview_df[\"Comments\"]\n",
    "            updated_df = sequence_overview_df.copy()\n",
    "            updated_df[\"Comments\"] = comment_column\n",
    "            os.remove(sequence_summary_path)\n",
    "            updated_df.to_csv(sequence_summary_path, sep = \"\\t\", index = False)\n",
    "            print(f\"Updated table at '{sequence_summary_path}'\")\n",
    "        else:\n",
    "            raise ValueError(f\"File '{sequence_summary_path}' exists. Change output path or flag to update to replace existing file.\")\n",
    "    else:\n",
    "            #leave as is\n",
    "            print(\"No changes detected. Updating not necessary.\")\n",
    "   \n",
    "    \n",
    "#If you get the error \"the process cannot access the file\", you probably have it open in some editor :) please close it so it can be overwritten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or overwrote file at '/home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/scripts/data/PROJECT1_scan_sessions_sequence_type_count.tsv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "OVERWRITE = True #should existing file be overwritten?\n",
    "# ==============================================\n",
    "\n",
    "description_count_filepath = os.path.abspath(os.path.join(OUTPUT_DIR, COUNT_FILENAME))\n",
    "if not os.path.exists(description_count_filepath) or OVERWRITE:\n",
    "    description_counts.to_csv(description_count_filepath, sep = \"\\t\", index = False)\n",
    "    print(f\"Created or overwrote file at '{description_count_filepath}'\")\n",
    "else:\n",
    "    raise ValueError(f\"File '{description_count_filepath}' exists. Change output path or flag to overwrite to replace existing file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modality rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or overwrote file at '/home/luisass/Work/drive_backup_git/MR-analysis/MR-Sequence-inspection/scripts/data/PROJECT1_scan_sessions_modality_rules.txt'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "OVERWRITE = True #should existing file be overwritten?\n",
    "MODALITY_RULES_FILENAME = fr'{IDENTIFIER}_scan_sessions_modality_rules.txt'\n",
    "# ==============================================\n",
    "\n",
    "modality_rules_to_text = list()\n",
    "modality_rules_to_text.append(\"Rules used to determine if a scan exists for a certain modality and session.\\\n",
    "                              \\nThe 'series_description' field in the dicom header is used as reference.\\n\")\n",
    "for v in MODALITY_CONDITIONS.values():\n",
    "    info = \"\".join(inspect.getsourcelines(v)[0][0].split(\"\\\"\")[1:])\n",
    "    info = info.replace(\",\\n\",\"\")\n",
    "    modality_rules_to_text.append(info)\n",
    "\n",
    "\n",
    "rule_filepath = os.path.abspath(os.path.join(OUTPUT_DIR, MODALITY_RULES_FILENAME))\n",
    "if not os.path.exists(rule_filepath) or OVERWRITE:\n",
    "    with open(rule_filepath, 'w') as f:\n",
    "        for line in modality_rules_to_text:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    print(f\"Created or overwrote file at '{rule_filepath}'\")\n",
    "else:\n",
    "    raise ValueError(f\"File '{rule_filepath}' exists. Change output path or flag to overwrite to replace existing file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
